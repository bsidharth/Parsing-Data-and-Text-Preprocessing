{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Text Preprocessing\n",
    "Environment: Python 3.6.5 and Jupyter notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "\n",
    "In this task, we are given a dataset of around 250 CVs of students,from which we need to generate a sparse representation of the resumes by using the given regular expression for tokenization and vocablary generation. We also need to generate a sparse matrix to keep a track of all  the words in the vocabulary in each resume.\n",
    "\n",
    "Steps followed for text preprocessing:\n",
    "\n",
    "1. Loading the resumes and storing it in the form of a dictionary, with key as the resume number and its data as its value.      Normalisation of the first character in the string is also done by senetence tokenising the string.\n",
    "\n",
    "2. After normalising I use the given regular expression and word tokenise each of the resumes, maintaining the dictionary    structure to avoid loosing the resume number of the resume.\n",
    "\n",
    "3. After that, I remove context independent stopwords from the tokens of each resume. \n",
    "\n",
    "4. It is followed by finding the top 200 bigrams that occur in the text and create a collocation vocabulary, which contains a combination of the unigrams as the bigrams and storing it in a new dictionary with the help of MWE Tokenizer. Still the resume number is preserved in the key  of the dictionary.\n",
    "\n",
    "5. Porter stemming is used to stem all the suffixes from the tokens and reduce the size of the vocabulary\n",
    "\n",
    "6. Followed by porterstemmer(), context dependent stopwords, i.e. the words that occur in too many or too few documents are removed from the vocabulary\n",
    "\n",
    "7. Words that are less than 3 letters are removed\n",
    "\n",
    "8. A final vocabulary is formed and a sparse matrix is created that keeps a track of each word in the vocabulary to the number of times it occurs in the resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Importing the required packages for task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sidha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from itertools import chain\n",
    "import itertools\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.probability import *\n",
    "nltk.download('punkt')\n",
    "from nltk.collocations import BigramCollocationFinder \n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from itertools import chain\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dictionary of resume numbers allocated with its data \n",
    "\n",
    "In this step we make a dictionary of all the resumes, with key as the resume numbers present in 'resume_dataset.txt' that are assigned to us, with the data in the resume as its value. We split the data in a resume by a '.' to normalize the first character of the string. After normalising the data we join the string again and assign it to its repective key in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filepath at which the resumes are present\n",
    "file_path = 'C:/Users/sidha/Desktop/resumeTxt'\n",
    "\n",
    "#Opening the given regualar expression \n",
    "fp = open('resume_dataset.txt') #opeing the file\n",
    "resume_dataset = fp.read() #reading from the file\n",
    "resume_dataset = resume_dataset.replace('\\n',' ') #removing the line breaker \n",
    "resume_dataset = resume_dataset.replace(',',' ') #removing ',' \n",
    "resume_dataset = list(resume_dataset[1:-1].split(\" \")) #forming a list by spliting the string at the whiteapce, after removing the square brackets at both ends\n",
    "\n",
    "#A empty list to store all the resumes  \n",
    "resume_dict={}\n",
    "\n",
    "for each in resume_dataset: #looping through each file in the folder\n",
    "    file_name = 'resume_('+each+').txt' #forming the file name\n",
    "    xfile = os.path.join(file_path, file_name) #forming the file path to file name\n",
    "    if os.path.isfile(xfile): #if the file exist\n",
    "        file_pointer = open(xfile,encoding=\"utf8\") #open the file to read\n",
    "        data = file_pointer.read() #read the full content of the file to a variable\n",
    "        if data: #if the file is not empty\n",
    "            data = data.replace('\\n','') #remove the line breakers to form a single line\n",
    "            data_list=str.split(data,'. ') \n",
    "            normalized_data_list= list(map(lambda w: w[: w.find(' ')].lower() + w[w.find(' '):] if w else '', data_list)) #normalizing the data to lower case\n",
    "            normalized_data = ' '.join(normalized_data_list)\n",
    "            resume_dict[each] = normalized_data #saving the normalized data from each document to a dictionary with resume name as the key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tokenizing words using the given regular expression\n",
    "\n",
    "In this step, we tokenize each resume in the dictionary with the help of the given regular expression. Then we form a dictionary of all the resumes with tokenized data in its values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The given regular expression\n",
    "tokenizer = RegexpTokenizer(r\"\\w+(?:[-']\\w+)?\") \n",
    "\n",
    "\n",
    "#To tokenize the data read from the reumes\n",
    "def tokenize(resume):\n",
    "    tokenized_resumes = tokenizer.tokenize(resume_dict[resume]) #tokenizing the string\n",
    "    return (resume, tokenized_resumes) # return a tupel of patent_id and a list of tokens\n",
    "\n",
    "#A dictionary with the resume numbers as the key and its value in the form of tokens\n",
    "tokenized_resumes = dict(tokenize(resume) for resume in resume_dict.keys()) #calling the tokenize method in a loop for all the elements in the dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Removing  tokens that are from  the given context independent stopwords from the vocab \n",
    "\n",
    "In this step, we remove all the tokens from the dictionary that are present in the given 'stopwords_en.txt' file. The purpose of this step is to reduce the vocab size as these stopwords do not reveal much information about the text and occur too often in the text, which makes other words in the vocabulary less significant. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An empty list to store all the given stopwords\n",
    "stopwords=[]\n",
    "\n",
    "#Opening the given stopwords file and storing the words in the stopwords list\n",
    "with open('stopwords_en.txt') as f:\n",
    "    stopwords = f.read().splitlines()    \n",
    "\n",
    "#Looping to remove all the context independent words from the dictionary\n",
    "for resume in tokenized_resumes.values():  \n",
    "    for word in resume:  \n",
    "        if word.lower() in stopwords:\n",
    "            resume.remove(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  6. Finding the top 200 Bigrams in the resumes\n",
    "\n",
    "In this step we find out the top 200 bigrams present in the text. Bigrams is a sequence of words that make completely different meaning when they are considered individually rather than together.\n",
    "\n",
    "Here, we use nltk.collocations.BigramAssocMeasures() to get the Bigram measure and nltk.collocations.BigramCollocationFinder.from_words() with nbest to find the top 200 bigrams usning liklihood_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating all the tokenized values using the chain.frome_iterable function to create a list of all the words \n",
    "total_tokens = list(chain.from_iterable(tokenized_resumes.values())) \n",
    "\n",
    "#Finding the top 200 bigrams\n",
    "finder=BigramCollocationFinder.from_words(total_tokens)\n",
    "bigrams=finder.nbest(BigramAssocMeasures.likelihood_ratio, 208)\n",
    "\n",
    "#Eliminating numbers from bigrams\n",
    "bigrams_list=[x for x in bigrams if not any(c.isdigit() for c in x)] \n",
    "\n",
    "#Preserving these bigrams and putting it back in the dictionary, along with the unigrams\n",
    "mwetokenizer = MWETokenizer(bigrams_list)\n",
    "\n",
    "#colloc_resumes is a dictionary that contains both the bigrams as well as the unigrams\n",
    "colloc_resumes =  dict((resume, mwetokenizer.tokenize(data)) for resume,data in tokenized_resumes.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Porter stemmmer\n",
    "\n",
    "In this step, we use the porter stemmer package to stem all the words by removing the suffixes from the tokens. This helps us in reducing the vocabulary size as the words with similar meaning are stemmed to a single word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the porterstemmer method\n",
    "ps=PorterStemmer()\n",
    "#An empty string to store the content of a particular resume\n",
    "strcontent=''\n",
    "#An empty dictionary to append the stemmed data back \n",
    "stemmed_dict=dict()\n",
    "\n",
    "#Looping to stem each value in the dictionary\n",
    "for key,resume in colloc_resumes.items():  \n",
    "    for word in resume:  \n",
    "        #Temporarily storing the data in an empty string\n",
    "        strcontent=strcontent+ ' ' + ps.stem(word)\n",
    "    \n",
    "    #Assigning the string to the respective key\n",
    "    stemmed_dict[key]=strcontent\n",
    "    #Again emptying the string to store the next resume\n",
    "    strcontent=''\n",
    "\n",
    "#Loop to again word tokenize each resume in the dictionary and assigning it back to its resume number \n",
    "for key,resume in stemmed_dict.items():\n",
    "    stemmed_dict[key]=word_tokenize(resume)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.  Removing context independent words\n",
    "\n",
    "Even after removing the context independent stopwords, there are some words that add little perspective to text preprocessing as they may be occuring in too many or too few documents to consider them in your vocabulary. Hence, in this step we remove words that occur only in 2% or over 98% of the total number of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An empty list to store the resume ids\n",
    "resumes = []\n",
    "#An empty list to store all the resume content in a singe list\n",
    "resume_words = []\n",
    "#An empty string to assign all the content\n",
    "txt = ''\n",
    "#Looping to append all the resume content in the empty sring\n",
    "for resume, tokens in stemmed_dict.items():\n",
    "    resumes.append(resume)\n",
    "    txt = ' '.join(tokens)\n",
    "    resume_words.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216, 2160)\n"
     ]
    }
   ],
   "source": [
    "# Using count vectorizer to remove words that occur in more than 98% and less than 2% of the resumes\n",
    "vectorizer = CountVectorizer(input = 'content', analyzer = 'word',max_df=0.98, min_df=0.02)\n",
    "vectorizerobject = vectorizer.fit_transform(resume_words)\n",
    "\n",
    "#Vocab contains a list of words after removing the context dependent stopwords\n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "#Checking the shape of the vectorizer obejct\n",
    "print(vectorizerobject.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Removing words that are less than 3 letters from the vocabulary\n",
    "\n",
    "A subset of the vocabulary is created which contains the words that are greater than 2 letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LENGTH OF THE VOCABULARY: 2020\n"
     ]
    }
   ],
   "source": [
    "#Using a list comprehension to elimenate 1 and 2 letter words\n",
    "vocab2=[word for word in vocab if len(word)>2]\n",
    "\n",
    "#Getting the length of the resultant vocabulary\n",
    "print('LENGTH OF THE VOCABULARY: ' + str(len(vocab2)))\n",
    "vocab_file = open(\"29330750_vocab.txt\", 'w')\n",
    "for word in vocab2:\n",
    "    vocab_file.write(word +  '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Creating a sparse matrix to keep a track of occurance of all the words in vocabulary in each resume\n",
    "\n",
    "The sparse matrix keeps a count of all the word occurances in vocabulary in each of the given resumes. It helps in keeping a track of diverity of the resultant vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initalising a file to write the sparse matrix\n",
    "save_file = open(\"29330750_countVec.txt\", 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return the coordinate representation of a sparse matrix\n",
    "cx = vectorizerobject.tocoo() \n",
    "for i,j,v in itertools.zip_longest(cx.row, cx.col, cx.data):\n",
    "    save_file.write(resumes[i] + ',' + vocab[j] + ',' + str(v) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
